{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es un Markov Decision Process (MDP)?  \n",
    "Un Markov decision process es un proceso en el cual se toman decisiones estocásticas que utiliza un marco matemático para modelar la toma de decisiones de un sistema dinámico en escenarios donde los resultados son aleatorios o controlados por un tomador de decisiones, este es el quien toma decisiones secuenciales a lo largo del tiempo.\n",
    "\n",
    "2. ¿Cuáles son los componentes principales de un MDP?\n",
    "A.\tEstados (S): Son un conjunto de todos los estados posibles en los que el entorno puede encontrarse.  \n",
    "B.\tAcciones (A): Son un conjunto de todas las acciones posibles que el agente puede tomar.  \n",
    "C.\tFunción de transición de estados (P): Describe la probabilidad de transición de un estado a otro dado una acción, formalmente es la probabilidad de que el entorno transiciones al estado s’ cuando el agente realiza la acción a en el estado s.  \n",
    "D.\tRecompensa (R): Esta es una función que asigna una recompensa inmediata a cada par de estados y acción.  \n",
    "E.\tDescuento (y): Este es un factor de descuento entre 0 y 1 que representa la importancia de las recompensas futuras en comparación de las recompensas inmediatas.  \n",
    "\n",
    "3. ¿Cuál es el objetivo principal del aprendizaje por refuerzo con MDPs?   \n",
    "El objetivo principal del aprendizaje por refuerzo con los MDP es encontrar una política optima que maximice la recompensa acumulada a lo largo del plazo para el agente, también esta política se puede definir como la estrategia o conjunto de reglas que el agente sigue para decidir que acciones tomar en cada estado, la política optima es aquella que maximiza la recompensa acumulada a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina los estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = {x for x in range(0, 9)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = 2\n",
    "obstacles = [4, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades de transición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de las probabilidades de transición y recompensas\n",
    "P = {s: {a: np.zeros(len(S)) for a in A} for s in S}\n",
    "R = {s: {a: np.zeros(len(S)) for a in A} for s in S}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0]['up'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[0]['down'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[0]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[0]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 1\n",
    "P[1]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[1]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[1]['left'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[1]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 2\n",
    "P[2]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[2]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[2]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[2]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 3\n",
    "P[3]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[3]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[3]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[3]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 4\n",
    "P[4]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[4]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[4]['left'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[4]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 5\n",
    "P[5]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[5]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[5]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[5]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 6\n",
    "P[6]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[6]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[6]['left'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[6]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 7\n",
    "P[7]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[7]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[7]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 8\n",
    "P[8]['up'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[8]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['left'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[8]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'up': [0, 0, 0, 1, 0, 0, 0, 0, 0], 'down': [0, 0, 0, 0, 1, 0, 0, 0, 0], 'left': [0, 0, 0, 0, 0, 0, 1, 0, 0], 'right': [0, 0, 0, 0, 0, 1, 0, 0, 0]}, 1: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 1, 0, 0, 0, 0, 0], 'right': [0, 0, 0, 0, 1, 0, 0, 0, 0]}, 2: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 0, 0, 0, 1, 0, 0], 'right': [0, 0, 0, 0, 0, 1, 0, 0, 0]}, 3: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 0, 0, 0, 1, 0, 0], 'right': [0, 0, 0, 0, 0, 1, 0, 0, 0]}, 4: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 1, 0, 0, 0, 0, 0], 'right': [0, 0, 0, 0, 1, 0, 0, 0, 0]}, 5: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 0, 0, 0, 1, 0, 0], 'right': [0, 0, 0, 0, 0, 1, 0, 0, 0]}, 6: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 1, 0, 0, 0, 0, 0], 'right': [0, 0, 0, 0, 1, 0, 0, 0, 0]}, 7: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 0, 0, 0, 1, 0, 0], 'right': [0, 0, 0, 0, 0, 1, 0, 0, 0]}, 8: {'up': [0, 0, 0, 0, 0, 0, 0, 1, 0], 'down': [0, 0, 0, 0, 0, 0, 0, 0, 1], 'left': [0, 0, 0, 1, 0, 0, 0, 0, 0], 'right': [0, 0, 0, 0, 1, 0, 0, 0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 1: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 2: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 3: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 4: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 5: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 6: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 7: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 8: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}}\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#       [ 1   2   3]\n",
    "#       [ 4   5   6]\n",
    "#       [ 7   8   9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(s, a):\n",
    "    if a == 'up':\n",
    "        return s - 3 if s >= 3 else s\n",
    "    elif a == 'down':\n",
    "        return s + 3 if s <= 5 else s\n",
    "    elif a == 'left':\n",
    "        return s - 1 if s % 3 != 0 else s\n",
    "    elif a == 'right':\n",
    "        return s + 1 if s % 3 != 2 else s\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in S:\n",
    "    for a in A:\n",
    "        s_prime = move(s, a)\n",
    "        # P[s][a][s_prime] = 1  # Probabilidad de transición determinística\n",
    "        if s_prime == goal_state:\n",
    "            R[s][a][s_prime] = 10  # Recompensa por alcanzar la meta\n",
    "        elif s_prime in obstacles:\n",
    "            R[s][a][s_prime] = -10  # Penalización por chocar con un obstáculo\n",
    "        elif s != s_prime:\n",
    "            R[s][a][s_prime] = -1  # Penalización por moverse a una celda normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de una política\n",
    "policy = {s: random.choice(A) for s in S}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(policy, steps=100):\n",
    "    total_reward = 0\n",
    "    state = 0  # Estado inicial\n",
    "    for _ in range(steps):\n",
    "        action = policy[state]\n",
    "        next_state = move(state, action)\n",
    "        reward = R[state][action][next_state]\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if state == goal_state:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar la política\n",
    "def evaluate_policy(policy, num_simulations=100, steps_per_simulation=100):\n",
    "    rewards = [simulate_policy(policy, steps_per_simulation) for _ in range(num_simulations)]\n",
    "    avg_reward = np.mean(rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política:\n",
      "Estado 0: right\n",
      "Estado 1: left\n",
      "Estado 2: up\n",
      "Estado 3: left\n",
      "Estado 4: down\n",
      "Estado 5: down\n",
      "Estado 6: left\n",
      "Estado 7: down\n",
      "Estado 8: down\n",
      "\n",
      "Recompensa acumulada promedio: -100.0\n"
     ]
    }
   ],
   "source": [
    "# Imprimir la política y la recompensa promedio\n",
    "print(\"Política:\")\n",
    "for s in S:\n",
    "    print(f\"Estado {s}: {policy[s]}\")\n",
    "\n",
    "avg_reward = evaluate_policy(policy, num_simulations=100, steps_per_simulation=100)\n",
    "print(f\"\\nRecompensa acumulada promedio: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado 0, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=1, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 0, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=-1.0\n",
      "  -> Estado 4 P=1, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 0, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=1, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 0, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=-1.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=1, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 1, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 1, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=-10.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=0.0\n",
      "Estado 1, Acción left:\n",
      "  -> Estado 0 P=0, R=-1.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=1, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 1, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=10.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=1, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 2, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=10.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 2, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=-1.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=0.0\n",
      "Estado 2, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=-1.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=1, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 2, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=10.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=1, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 3, Acción up:\n",
      "  -> Estado 0 P=0, R=-1.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 3, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=-1.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=0.0\n",
      "Estado 3, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=1, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 3, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=-10.0\n",
      "  -> Estado 5 P=1, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 4, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=-1.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 4, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=-1.0\n",
      "  -> Estado 8 P=1, R=0.0\n",
      "Estado 4, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=1, R=-1.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 4, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=1, R=0.0\n",
      "  -> Estado 5 P=0, R=-1.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 5, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=10.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 5, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=-10.0\n",
      "Estado 5, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=-10.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=1, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 5, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=1, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 6, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=-1.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 6, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=0.0\n",
      "Estado 6, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=1, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 6, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=1, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=-1.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 7, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=-10.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 7, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=0.0\n",
      "Estado 7, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=1, R=-1.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 7, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=1, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=-10.0\n",
      "Estado 8, Acción up:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=-1.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=1, R=0.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 8, Acción down:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=1, R=-10.0\n",
      "Estado 8, Acción left:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=1, R=0.0\n",
      "  -> Estado 4 P=0, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=-1.0\n",
      "  -> Estado 8 P=0, R=0.0\n",
      "Estado 8, Acción right:\n",
      "  -> Estado 0 P=0, R=0.0\n",
      "  -> Estado 1 P=0, R=0.0\n",
      "  -> Estado 2 P=0, R=0.0\n",
      "  -> Estado 3 P=0, R=0.0\n",
      "  -> Estado 4 P=1, R=0.0\n",
      "  -> Estado 5 P=0, R=0.0\n",
      "  -> Estado 6 P=0, R=0.0\n",
      "  -> Estado 7 P=0, R=0.0\n",
      "  -> Estado 8 P=0, R=-10.0\n"
     ]
    }
   ],
   "source": [
    "for s in P:\n",
    "    for a in P[s]:\n",
    "        print(f\"Estado {s}, Acción {a}:\")\n",
    "        for s_prime in range(len(P[s][a])):\n",
    "            print(f\"  -> Estado {s_prime} P={P[s][a][s_prime]}, R={R[s][a][s_prime]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "En clase hemos dicho que una vez tengamos v* o q* sabemos la póliza óptima π* ¿Por qué?\n",
    "\n",
    "Si tenemos v* las mejores acciones que se pueden tomar en cada estado son las que maximizan la recompensa esperada en el tiempo inmediato. En otras palabras, la política óptima es aquella que es greedy con respecto a v*. Esto quiere decir, que si se evalúan las consecuencias de tomar una acción acorde a v* en el corto plazo, sin considerar el largo plazo, especificamente al siguiente timestep, entonces la política greedy, que elige las acciones que maximizan el reward en el corto plazo, es óptima.\n",
    "\n",
    "Si tenemos q* solo es necesario buscar cualquier acción que maximice q*(s, a). La función acción-valor nos dice cuál es la mejor acción a tomar en cada estado, por lo que la política óptima es aquella que sigue las acciones que maximizan q*(s, a) en cada estado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
